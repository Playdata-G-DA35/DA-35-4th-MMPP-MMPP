{"cells":[{"cell_type":"code","source":["# 필요한 패키지 설치\n","!pip install -q --upgrade diffusers transformers accelerate peft"],"metadata":{"id":"LCKWXsPVsh4G"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I_HOR6t2yZ3X"},"outputs":[],"source":["# 허깅 페이스 로그인\n","!huggingface-cli login"]},{"cell_type":"markdown","source":["# Interference with LoRA\n","- canny controlnet은 U-net에서 Canny Edge 또한\n","- 이미지 생성을 위한 Denoising의 Condtion으로 활용할 수 있도록 한다.\n","- 즉, 생성될 이미지의 형태를 Edge로서 제어 가능하다.\n","- 얼굴의 모양적 특성을 기억하기 위해 사용했다."],"metadata":{"id":"T5T5ydiNrsIP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mhHOJux7OcaJ"},"outputs":[],"source":["from diffusers import ControlNetModel, AutoencoderKL, StableDiffusionXLControlNetImg2ImgPipeline\n","from diffusers.utils import load_image, make_image_grid\n","from PIL import Image\n","import cv2\n","import numpy as np\n","import torch\n","\n","# orginal_image < 변환을 시작할 이미지\n","# load_image (이미지 경로나 url)\n","url = \"\"\n","original_image = load_image(url)\n","image = np.array(original_image)\n","\n","# 엣지 검출 Threshold 설정\n","low_threshold = 70\n","high_threshold = 130\n","\n","# Canny Edge 생성\n","image = cv2.Canny(image, low_threshold, high_threshold)\n","image = image[:, :, None]\n","image = np.concatenate([image, image, image], axis=2)\n","canny_image = Image.fromarray(image)\n","make_image_grid([original_image, canny_image], rows=1, cols=2)"]},{"cell_type":"code","source":["# canny controlnet 모델 설정\n","# canny controlnet은 U-net에서 Canny Edge 또한\n","# 이미지 생성을 위한 Denoising의 Condtion으로 활용할 수 있도록 한다.\n","# 얼굴의 모양적 특성을 기억하기 위해 사용.\n","controlnet = ControlNetModel.from_pretrained(\n","    \"diffusers/controlnet-canny-sdxl-1.0\",\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",")\n","# VAE 설정\n","vae = AutoencoderKL.from_pretrained(\"madebyollin/sdxl-vae-fp16-fix\", torch_dtype=torch.float16, use_safetensors=True)\n","# SDXL Controlnet Pipeline 생성\n","pipe = StableDiffusionXLControlNetImg2ImgPipeline.from_pretrained(\n","    \"stabilityai/stable-diffusion-xl-base-1.0\",\n","    controlnet=controlnet,\n","    vae=vae,\n","    torch_dtype=torch.float16,\n","    use_safetensors=True\n",").to(\"cuda\")"],"metadata":{"id":"C1q8R7UyvENM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[" # 기본 프로필 모델 LoRA를 불러온다.\n"," # 사용할 스타일 LoRA의 영향력이 지나치지 않도록 하고\n"," # 프로필 사진 형태의 이미지가 생성되도록 유도한다.\n","pipe.load_lora_weights(\"seoheelee/bpp_lora_small\", weight_name=\"pytorch_lora_weights.safetensors\", adapter_name=\"bpp\")\n","\n","# 원하는 스타일 LoRA를 불러온다.\n","pipe.load_lora_weights(\"YOURLORA\", weight_name=\".safetensors\", adapter_name=\"YOURLORA\")\n","\n","# 각각 반영 비율(adapter_weights), bpp의 비율은 생성될 이미지를 실제 사진에 가깝게 만든다.\n","# 즉, 실제 사진과 거리가 먼 모델일 수록 비율을 어느정도 줄여야 성능이 좋다.\n","pipe.set_adapters([\"bpp\", \"YOURLORA\"], adapter_weights=[1.0, 1.0])\n","\n","# 기본 모델의 U-net에 내가 설정한 Adapter를 합친다.\n","pipe.fuse_lora(adapter_names=[\"bpp\", \"YOURLORA\"], lora_scale=1.0) # lora scale << 총 반영비율 (0.9나 1.0)\n","\n","# 합쳤으므로 불러왔던 LoRA weight는 내려준다\n","pipe.unload_lora_weights()"],"metadata":{"id":"zM45FnUFuEoE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LoRA가 학습에 사용한 instance prompt가 있을시(readme.md에서 확인가능) Trigger로서 prompt에 추가 해줘야 한다.\n","# 사용된 얼굴 이미지의 성별, 연령대 등 특징을 포함할 시 더 정확한 이미지를 생성한다.\n","# 이 외에도, 생성될 스타일에 맞게 추가적으로 프롬프트를 설정한다.\n","# 중요한 것일 수록 앞에 적으며 Theme-Style-Detail 순으로 적어주는 것이 좋다.\n","# 나타나지 않길 바라는 특징은 부정 프롬프트로 설정한다.\n","prompt = \"YOURTK, a mmpp of USERDATA\"\n","negative_prompt = \"colorful, bad quality, ugly, bad anotomy, unrealistic, bad image, noisy\"\n","\n","image = pipe(\n","    prompt=prompt,\n","    negative_prompt=negative_prompt,\n","    image=original_image, # 시작 이미지\n","    control_image=canny_image, # 시작 이미지의 Canny Image\n","    controlnet_conditioning_scale=0.5, # Canny Condtioning의 반영 비율\n","    strength = 0.90, # 시작 이미지의 몇퍼센트를 Noise로 인식하느냐를 의미한다. 즉, 높을수록 원본과 많이 달라질 수 있으며 일반적으로 0.8 이상이 권장된다.\n","    guidance_scale=10.0 # 프롬프트 반영 비율 (보통 9~15 정도가 적절)\n",").images[0]\n","image"],"metadata":{"id":"TxXIXZ9WsKCF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# fuse lora를 통해 기본 모델과 LoRA가 결합되었으므로\n","# 파라미터들이 최종적으로 결정됐다면 모델 전체를 저장할 수 있다.\n","# 확장성, 유연성의 문제로 정말 성능이 매우 높지 않는 한 추천하지 않음.\n","pipe.push_to_hub(\"\")"],"metadata":{"id":"zYMcpjSjuOLM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# IP Adapter\n","- Style LoRA를 활용할 뿐만 아니라,\n","- 임의의 이미지를 스타일로서 지정할 수 있다.\n","- 즉, 제공한 이미지와 같은 스타일 + 내가 제공한 조건으로 이미지를 생성할 수 있다.\n","- 서비스 제공 환경에서는 다른 유저가 생성한 이미지를 참조할 이미지로 사용하는 것을 고려함."],"metadata":{"id":"kBfPYoIjaKE6"}},{"cell_type":"code","source":["from diffusers import AutoPipelineForImage2Image\n","from diffusers.utils import load_image\n","import torch\n","\n","# image to image pipeline을 사용하였다.\n","# 반영할 이미지의 스타일을 살리기 위해 Canny는 사용하지 않음.\n","# 필요하다면 Controllnet을 사용한다.\n","ia_pipeline = AutoPipelineForImage2Image.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float16).to(\"cuda\")\n","ia_pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"sdxl_models\", weight_name=\"ip-adapter_sdxl.bin\")\n","\n","\n","# set_ip_adapter_scale은 0~1 사이의 실수로 정할 수도 있지만\n","# U-net의 블록 별로도 지정하면서 테스트해볼 수 있다. U-net은 Down Sampler와 Up Sampler로 이루어져있다.\n","# down : down sampler, 이미지 크기를 축소하면서 이미지의 구조적 특징 추출 (layout block)\n","# up : up sampler, 이미지 크기를 복원하면서 디테일 정보 추가 (style block)\n","ia_scale = {\n","    \"down\": {\"block_2\": [0.0, 1.0]}, # down sampler의 세번째 블록 두번째 트랜스포머에만 반영\n","    \"up\": {\"block_0\": [0.0, 1.0, 0.0]}, # down sampler의 첫번째 블록 두번째 트랜스포머에만 반영\n","}\n","ia_pipeline.set_ip_adapter_scale(ia_scale)"],"metadata":{"id":"pX3yb9YmQWCe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# LoRA를 이용해 생성할 이미지의 스타일을 추가적으로 지정할 수 있다.\n","pipe.load_lora_weights(\"seoheelee/bpp_lora_small\", weight_name=\"pytorch_lora_weights.safetensors\", adapter_name=\"bpp\")\n","\n","# 원하는 스타일 LoRA를 불러온다.\n","pipe.load_lora_weights(\"YOURLORA\", weight_name=\".safetensors\", adapter_name=\"YOURLORA\")\n","\n","# 각각 반영 비율(adapter_weights), bpp의 비율은 생성될 이미지를 실제 사진에 가깝게 만든다.\n","# 즉, 실제 사진과 거리가 먼 모델일 수록 비율을 어느정도 줄여야 성능이 좋다.\n","pipe.set_adapters([\"bpp\", \"YOURLORA\"], adapter_weights=[1.0, 1.0])\n","\n","# 기본 모델의 U-net에 내가 설정한 Adapter를 합친다.\n","pipe.fuse_lora(adapter_names=[\"bpp\", \"YOURLORA\"], lora_scale=1.0) # lora scale << 총 반영비율 (0.9나 1.0)\n","ia_pipeline.unload_lora_weights()"],"metadata":{"id":"9YrMbfzoRKlE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 스타일로 반영할 이미지\n","url1 = \"\"\n","style_image = load_image(url1)\n","# 시작 이미지\n","url2 = \"\"\n","original_image = load_image(url2)\n","\n","# IP Adapter는 영향력이 강하므로, 이를 이용해 이미지를 생성할 때는\n","# ip adapter scale을 잘 지정해주거나\n","# 원본 이미지의 정보를 지키기 위해 프롬프트를 구체적으로 설정해주는 것이 좋다.\n","image = ia_pipeline(\n","    prompt=\"YOURTK, a mmpp of USERDATA\",\n","    image=original_image,\n","    ip_adapter_image=style_image,\n","    negative_prompt=\"3d render, monochrom, bad quality, bad anotomy, unrealistic, low quality, worst quality, deformed, glitch, low contrast, noisy\",\n","    strength=0.85,\n","    guidance_scale=10,\n",").images[0]\n","image"],"metadata":{"id":"-NhuKYnraH2P"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"mount_file_id":"1Mh0i_0asZWQA1U26rgwze109agwNZMZ8","authorship_tag":"ABX9TyPJbnrESN05BgCQrwBL94HO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}